{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsCFWeIt56VW"
      },
      "source": [
        "# Course 2: Sentiment Analysis using Logistic Regression\n",
        "In this project, I will build a sentiment analysis model using Logistic Regression:\n",
        "\n",
        "* Learn how to extract features for logistic regression given some text\n",
        "* Implement logistic regression using only mathematical formula without using any python library\n",
        "* Apply logistic regression on a natural language processing task\n",
        "* Validate your inference performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqggZXYL56VX"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "- [Import Libraries and Data](#0)\n",
        "- [1 - Extracting the Features](#1)\n",
        "- [2 - Logistic Regression](#1)\n",
        "    - [2.1 - Sigmoid function](#1-1)\n",
        "    - [2.2 - Cost function and Gradient](#1-2)\n",
        "- [3 - Training Your Model](#3)\n",
        "- [4 - Test your Logistic Regression](#4)\n",
        "    - [4.1 - Check the Performance using the Test Set](#4-1)\n",
        "- [5 - Predict with your own text](#5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkxCiFAu56VX"
      },
      "source": [
        "<a name='0'></a>\n",
        "## Import Libraries and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90s95IW256VX",
        "outputId": "908e150f-d92c-4be1-f13e-81ac8bfff52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n",
            "Downloads completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#import nltk\n",
        "import nltk\n",
        "from os import getcwd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "nltk.download()\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "print(\"Downloads completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pbNHSUD56VY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "dd7ec8ca-75d5-4c84-df45-b026a5f7f946"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d59164c8-95b8-4e26-b6ff-5dc895fb5ec2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d59164c8-95b8-4e26-b6ff-5dc895fb5ec2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d59164c8-95b8-4e26-b6ff-5dc895fb5ec2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d59164c8-95b8-4e26-b6ff-5dc895fb5ec2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-65bb2689-cfba-4793-a756-5a4f5bdcc454\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-65bb2689-cfba-4793-a756-5a4f5bdcc454')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-65bb2689-cfba-4793-a756-5a4f5bdcc454 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 49999,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49580,\n        \"samples\": [\n          \"This movie was a modern day scarface.It had me on my toes.This movie is one of those rare epic films that makes you want a sequel.I especially liked Damian Chapa his performance deserved an academy award,which he deserved for his performance in blood in blood out.The only thing I didn't like was the behind the scenes because it didn't show the intensity that the movie had,and i would have like to have seen less narrated scenes.But the movie was great and it is in my top ten movies of all time.Plus the acting was great there wasn't a bad scene in the movie,I loved it ,Jennifer Tilly was perfect as well as all of the cast.I can't see how anyone wouldn't like this movie it was a great.Definitely a must see.\",\n          \"This is probably one of the best French movies I had seen in a very long time! This \\\"pastiche\\\" or parody of spy movies is very well made and is going to make you laugh from the beginning to the end. Some references to today's world are very subtle. The whole Maroccan context of the movie is to be understood in light of today's French culture/environment. That said, all the jokes and - seemingly - shocking remarks that could have been understood as such because of this context, are permitted and accepted because this is a parody. <br /><br />I was told by my sisters who had already seen this movie that I should go too and assured me that I was going to have a great time, and indeed I had! If you liked the old 007 movies with Sean Connery and also like movies like Airplane or Hot Shots, you will be delighted. I just hope this movie is released on DVD in the US... Wait and see.\",\n          \"\\\"Giant\\\" is one of the most boring, overly-long Hollywood contraptions ever. Many scenes seem utterly fake and without energy. Rock Hudson, Elizabeth Taylor, and James Dean are wasted in this big Hollywood production. A central notion to this movie, that a rancher would ever resist drilling for oil on his land, is absurd, and I know this because I'm from Houston. A couple of scenes, especially Dean serving Taylor coffee, redeem this otherwise boring film. For a much more accurate and interesting depiction about how modernism changed the ranches in Texas, see \\\"Hud\\\" (one of Paul Newman's great performances) or \\\"The Last Picture Show.\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df = pd.read_csv('/content/IMDB.csv')\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EAXA47U56VY"
      },
      "source": [
        "### Prepare the Data\n",
        "\n",
        "* Train set 80% and test set 20%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwUm__9A56VY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad605cdb-22e6-4fca-87ab-f32185ce2022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train X shape: (39999,)\n",
            "Test X shape: (10000,)\n",
            "Train Y shape: (39999, 1)\n",
            "Test Y shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Encode labels: pos=1, neg=0\n",
        "df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "# Shuffle data\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split data\n",
        "train_x, test_x, train_y, test_y = train_test_split(\n",
        "    df['review'], df['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "train_x = np.array(train_x.tolist())\n",
        "test_x = np.array(test_x.tolist())\n",
        "train_y = np.array(train_y).reshape(-1, 1)\n",
        "test_y = np.array(test_y).reshape(-1, 1)\n",
        "\n",
        "print(\"Train X shape:\", train_x.shape)\n",
        "print(\"Test X shape:\", test_x.shape)\n",
        "\n",
        "\n",
        "print(\"Train Y shape:\", train_y.shape)\n",
        "print(\"Test Y shape:\", test_y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3VF8x_t8W7o",
        "outputId": "018db179-3be7-4096-e3fd-bf351a8b882c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['I haven\\'t seen this film since it came out in the mid 70s, but I do recall it as being a very realistic portrayal of the music business ( right up there with Paul Simons \"One Trick Pony \" ..another vastly underrated film IMO )<br /><br />Harvey Keitel does an excellent job as a producer caught between the music he believes in , and the commercial \"tripe\" the record company \"suits\" want him to work with.<br /><br />Since I spent my entire career in the music business as a composer /arranger /producer, I can really vouch for the verisimilitude this film possesses. <br /><br />If it should ever come out on DVD uncut, I\\'d buy it!',\n",
              "       \"I recently rented this movie as part of a nostalgic phase I'm going through. I was born in 1980, and so film from mid-80s to mid-90s has quite an important place in my growing up.<br /><br />This particular movie was one of my favourites, and so I was thrilled when it became available in the UK. It hasn't become worse with time, it is still a great fun film, with plenty of excitement in its own way. Sure, it pales in the shadow of bigger, larger budget films, but don't let that stop you enjoying this.<br /><br />Worth a rent, or even a purchase at the discount prices you'll find it for.\",\n",
              "       \"How do I describe the horrors?!!! First, some points: First, this review should be taken with a grain of salt -- I saw this over 20 years ago, when I was a boy, at the Museum of Modern Art in New York City.<br /><br />Secondly, I am giving away some scenes and plot points. However, it does not have much of a plot.<br /><br />Finally, I don't enjoy these type of art films anyway.<br /><br />This film was directed by proto-auteur Luis Bunuel. He was a surrealist and dadaist. These were modernist themes or movements popular critically in the 1920's and early 1930's. Surealism was the school of art that made things hyper-real, yet often had Freudian symbolism. Dadaism is based on what is supposedly the first word made by an infant -- Dada, or father.<br /><br />Made in black and white, it was also made by a band of communists (or as they preferred the term, socialists). Bunuel and his group of fellow film-makers and artistes had been working on a number of symbolic ideas and issues in Spain and France between the world wars.<br /><br />Dadaism and surrealism influenced a lot of artists -- The Police (Doo doo doo da), poet Arthur Rambaud, Edvard Munch (The Scream), Rene Magritte (floating hats in space), Salvador Dali (melting clocks), and even Hitchcock (Psycho). No Norman Rockwell.<br /><br />Here's what I recall most about this film: a girl meets up with a cow; her eye gets slashed by a razor; clownish men cavort in a meadow. There is not, as I said, much of a plot, but then again, that must be the point.<br /><br />This was attacked as porn back then, and would be again today. One of the trade-marks of surrealism is a significant anti-feminism.\",\n",
              "       ...,\n",
              "       'We just saw this movie in Austin Texas at the Alamo South Lamar yesterday afternoon. It had me laughing out loud many times! The scene about Albert Einstein\\'s thoughts on humanity hit me over and over and I couldn\\'t stop laughing. It\\'s too bad it\\'s not in more theaters, I know a lot of friends that are dieing to see this movie! \"Welcome to Costco, I love you.\" ... great work to all involved! Also, if you see it, make sure to stay until the end of the credits as well! I\\'m going to take my family to see it again this weekend for sure! If you\\'re a fan of OFFICE SPACE and BEAVIS AND BUTTHEAD then you have to go see this movie. It\\'s a classic and no one knows that it\\'s out! So if you\\'re in the mood to see something funny this weekend, definitely check it out.',\n",
              "       \"This is a catastrophe movie set in London . Starting multiple hurricane,superstorm and tornadoes on Scotland are displaced towards East , downing England coast and later the South. After several hours of heavy rainful , the London barrier above Thames is short from running over, and it paves the way for disaster. Then a colossal tidal-wave travel relentless down East causing devastation and lives of millions of Londoners are in danger. At the center of the story is a climatologist(Tom Courtenay) a climatologist who tries to save London from the effects of giant wave , trying to convince the authorities that the town dike was unsafe, furthermore a marine engineer (Robert Carlyle) and his ex-wife Samantha(Jessalyn Gilsig) . They are trapped into the barrier and dropped to sea .Meantime the secret government agency HQ ruled by Nash(Joanne Whalley) under direct orders of deputy Minister(David Suchet) attempt to control many displaced and avoid more dead, approximately 200.000. They have a little time to save London from total catastrophe.<br /><br />Perfectly acceptable drama-disaster with alright acting. Magnificent Tom Courtenay as a climatologist who predicts catastrophe and excellent Robert Carlyle and Jessalyn Gilsig as ex-matrimony rekindling their love. The movie packs impressive flood scenes brought to life by the breathtaking computer generator special effects, better than the classic of the 70s , such as 'Earthquake, Inferno Towering' and similarly to 'Armaguedon and Day after tomorrow'. Although isn't a clear denounce, we know that the flood is caused by the greenhouse effect and global warming which originates the ozone hole. The motion picture is well directed by Tony Mitchell. I would recommend this movie to people who like disaster movies. Another adaptations about floods, are the following : 'Flood(1976)'directed by Earl Bellamy with Robert Culp and Barbara Hershey; 'Hard rain(1998)' directed by Mikael Salomon with Morgan Freeman and Christian Slater; ' Flood : a river's rampage(1979)' directed by Bruce Pittman with Richard Thomas\",\n",
              "       \"This movie sucked. The acting sucked, the script sucked, and the movie overall sucked. There were two threads in the movie that were not developed and the viewer had to do a bit of work to figure out what was happening.<br /><br />I'm not saying that it needs to be spelled out, but you suddenly find things happening and being said as if you have the slightest clue as to what they are. Examples:<br /><br />The heroine's negative comments about the hero. The audience is never shown how she even knows anything about the guy and how he is tied into her fiance's death. The viewer has minimal exposure to the guy's death as well.<br /><br />Also, all of a sudden there is a scene with a bunch of guys loading up and cocking machine guns and that is all you see before cutting back to the other scenes. No explanation what-so-ever about the guns and the folks with them.<br /><br />We gave it a 3 because we didn't feel like we wanted our time back. It was fun to bad-mouth the movie while watching it, so it at least gave us a bit of entertainment. ;-)\"],\n",
              "      dtype='<U13704')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cB31ruV8Z-W",
        "outputId": "cda67ce7-7864-463c-9322-e71571177a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [1],\n",
              "       [0],\n",
              "       ...,\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WIObu6W56VZ"
      },
      "source": [
        "* Create the function for processing the string/text:\n",
        "    - tokenization.\n",
        "    - remove stop words.\n",
        "    - apply stemming.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9YPiiGp56VZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c90a1ceb-dbb9-41ff-cee6-e1f2e669264b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Preprocessing function\n",
        "def process_text(text):\n",
        "    \"\"\"Clean and preprocess IMDB review text.\"\"\"\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove hyperlinks\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags (like <br />)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # Remove stopwords and apply stemming\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    cleaned = [\n",
        "        stemmer.stem(word)\n",
        "        for word in tokens\n",
        "        if word not in stop_words and word.isalpha()\n",
        "    ]\n",
        "\n",
        "    return cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_text(train_x[100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI8vGVO78i_c",
        "outputId": "fb81defa-f722-4de4-e3f9-1876925eccb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['spoiler',\n",
              " 'film',\n",
              " 'noth',\n",
              " 'could',\n",
              " 'written',\n",
              " 'could',\n",
              " 'make',\n",
              " 'wors',\n",
              " 'dictionari',\n",
              " 'definit',\n",
              " 'pueril',\n",
              " 'read',\n",
              " 'sex',\n",
              " 'live',\n",
              " 'potato',\n",
              " 'men',\n",
              " 'unless',\n",
              " 'like',\n",
              " 'dog',\n",
              " 'poo',\n",
              " 'mucou',\n",
              " 'case',\n",
              " 'film',\n",
              " 'see',\n",
              " 'johnni',\n",
              " 'vega',\n",
              " 'et',\n",
              " 'think']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_clean = [' '.join(process_text(text)) for text in train_x]\n",
        "test_x_clean = [' '.join(process_text(text)) for text in test_x]\n"
      ],
      "metadata": {
        "id": "6sO2VeNQVyBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq79QOc656VZ"
      },
      "source": [
        "* Create the frequency dictionary function.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhd7Ke0r56VZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_freqs(texts, labels):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        texts  - a list/array of review texts\n",
        "        labels - a 2D numpy array of labels (0 or 1)\n",
        "    Output:\n",
        "        freqs  - dictionary of (word, label) → frequency\n",
        "    \"\"\"\n",
        "    # Flatten labels to 1D list\n",
        "    labels_list = np.squeeze(labels).tolist()\n",
        "\n",
        "    freqs = {}\n",
        "\n",
        "    # Loop through each (label, text) pair\n",
        "    for y, text in zip(labels_list, texts):\n",
        "        for word in process_text(text):\n",
        "            pair = (word, y)\n",
        "            freqs[pair] = freqs.get(pair, 0) + 1\n",
        "\n",
        "    return freqs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsWZq8Uz56Va",
        "outputId": "20c318e1-a24b-4007-97b2-913ed1f7769c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of freqs: <class 'dict'>\n",
            "Number of (word, label) pairs: 184828\n"
          ]
        }
      ],
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "print(\"Type of freqs:\", type(freqs))\n",
        "print(\"Number of (word, label) pairs:\", len(freqs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nDTnpz656Va"
      },
      "source": [
        "### Process Text\n",
        "The given function 'process_text' tokenizes the review into individual words, removes stop words and applies stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6w_Wdem56Va",
        "outputId": "bda6e1ac-8385-422d-d1a3-3759117bade3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original IMDB review:\n",
            " I haven't seen this film since it came out in the mid 70s, but I do recall it as being a very realistic portrayal of the music business ( right up there with Paul Simons \"One Trick Pony \" ..another vastly underrated film IMO )<br /><br />Harvey Keitel does an excellent job as a producer caught between the music he believes in , and the commercial \"tripe\" the record company \"suits\" want him to work with.<br /><br />Since I spent my entire career in the music business as a composer /arranger /producer, I can really vouch for the verisimilitude this film possesses. <br /><br />If it should ever come out on DVD uncut, I'd buy it!\n",
            "\n",
            "Processed version of the review:\n",
            " ['havent', 'seen', 'film', 'sinc', 'came', 'mid', 'recal', 'realist', 'portray', 'music', 'busi', 'right', 'paul', 'simon', 'one', 'trick', 'poni', 'anoth', 'vastli', 'underr', 'film', 'imo', 'harvey', 'keitel', 'excel', 'job', 'produc', 'caught', 'music', 'believ', 'commerci', 'tripe', 'record', 'compani', 'suit', 'want', 'work', 'withsinc', 'spent', 'entir', 'career', 'music', 'busi', 'compos', 'arrang', 'produc', 'realli', 'vouch', 'verisimilitud', 'film', 'possess', 'ever', 'come', 'dvd', 'uncut', 'id', 'buy']\n"
          ]
        }
      ],
      "source": [
        "# test the function below\n",
        "print('Original IMDB review:\\n', train_x[0])\n",
        "print('\\nProcessed version of the review:\\n', process_text(train_x[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D09v06so56Va"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Logistic Regression\n",
        "\n",
        "<a name='1-1'></a>\n",
        "### 1.1 - Sigmoid\n",
        "You will learn to use logistic regression for text classification.\n",
        "* The sigmoid function is defined as:\n",
        "\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
        "\n",
        "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability.\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='./images/sigmoid_plot.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:200px;\" /> Figure 1 </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUS8DrBS56Va"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    '''\n",
        "    Inputs:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Outputs:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "    # calculate the sigmoid of z\n",
        "    h = 1/(1+np.exp(-z))\n",
        "\n",
        "    return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzNkG2Yq56Va"
      },
      "source": [
        "#### Logistic Regression: Regression and a Sigmoid\n",
        "\n",
        "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
        "\n",
        "Regression:\n",
        "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "Note that the $\\theta$ values are \"weights\". If you took the deep learning specialization, we referred to the weights with the 'w' vector.  In this course, we're using a different variable $\\theta$ to refer to the weights.\n",
        "\n",
        "Logistic regression\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
        "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "We will refer to 'z' as the 'logits'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUuPxwfZ56Va"
      },
      "source": [
        "<a name='1-2'></a>\n",
        "### 1.2 - Cost function and Gradient\n",
        "\n",
        "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
        "* $m$ is the number of training examples\n",
        "* $y^{(i)}$ is the actual label of training example 'i'.\n",
        "* $h(z^{(i)})$ is the model's prediction for the training example 'i'.\n",
        "\n",
        "The loss function for a single training example is\n",
        "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
        "\n",
        "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
        "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label 'y' is also 1, the loss for that training example is 0.\n",
        "* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0.\n",
        "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_74ubNPe56Vb",
        "outputId": "3c147295-c8fc-4b31-f37b-42dbdba5964e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(9.210340371976294)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# verify that when the model predicts close to 1, but the actual label is 0, the loss is a large positive value\n",
        "-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXi09NLk56Vb"
      },
      "source": [
        "* Likewise, if the model predicts close to 0 ($h(z) = 0.0001$) but the actual label is 1, the first term in the loss function becomes a large number: $-1 \\times log(0.0001) \\approx 9.2$.  The closer the prediction is to zero, the larger the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDb0w6PH56Vb",
        "outputId": "f0c9b6f4-566c-4313-8036-fbf127dbc688"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(9.210340371976182)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# verify that when the model predicts close to 0 but the actual label is 1, the loss is a large positive value\n",
        "-1 * np.log(0.0001) # loss is about 9.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFWC6Mec56Vb"
      },
      "source": [
        "#### Update the weights\n",
        "\n",
        "To update your weight vector $\\theta$, you will apply gradient descent to iteratively improve your model's predictions.  \n",
        "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
        "\n",
        "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j \\tag{5}$$\n",
        "* 'i' is the index across all 'm' training examples.\n",
        "* 'j' is the index of the weight $\\theta_j$, so $x^{(i)}_j$ is the feature associated with weight $\\theta_j$\n",
        "\n",
        "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
        "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
        "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzj38p0W56Vb"
      },
      "source": [
        "<a name='ex-2'></a>\n",
        "### Exercise 2 - gradientDescent\n",
        "Implement gradient descent function.\n",
        "* The number of iterations 'num_iters\" is the number of times that you'll use the entire training set.\n",
        "* For each iteration, you'll calculate the cost function using all training examples (there are 'm' training examples), and for all features.\n",
        "* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
        "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
        "\\theta_0\n",
        "\\\\\n",
        "\\theta_1\n",
        "\\\\\n",
        "\\theta_2\n",
        "\\\\\n",
        "\\vdots\n",
        "\\\\\n",
        "\\theta_n\n",
        "\\end{pmatrix}$$\n",
        "* $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (note that the corresponding feature value $\\mathbf{x_0}$ is 1).\n",
        "* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
        "    * $\\mathbf{x}$ has dimensions (m, n+1)\n",
        "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
        "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
        "* The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n",
        "* The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'.  Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
        "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
        "* The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
        "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-fGfdji56Vb"
      },
      "outputs": [],
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Inputs:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Outputs:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    '''\n",
        "    # get 'm', the number of rows in matrix x\n",
        "    m = len(x)\n",
        "    for i in range(0, num_iters):\n",
        "\n",
        "        # get z, the dot product of x and theta\n",
        "        z = np.dot(x,theta)\n",
        "\n",
        "        # get the sigmoid of z\n",
        "        h = sigmoid(z)\n",
        "\n",
        "        # calculate the cost function\n",
        "        J = -1/m*(np.dot(y.transpose(),np.log(h))+np.dot((1-y).transpose(),np.log(1-h)))\n",
        "\n",
        "        # update the weights theta\n",
        "        theta = theta -alpha/m*(np.dot(x.transpose(),(h-y)))\n",
        "    J = float(J)\n",
        "    return J, theta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ2dES9h56Vb",
        "outputId": "90f3d611-88f1-4fa0-98c2-dfb13c87c47f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.67094970.\n",
            "The resulting vector of weights is [np.float64(4.1e-07), np.float64(0.00035658), np.float64(7.309e-05)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-87cfadf101d0>:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  J = float(J)\n"
          ]
        }
      ],
      "source": [
        "# Check the function\n",
        "# Construct a synthetic test case using numpy PRNG functions\n",
        "np.random.seed(1)\n",
        "# X input is 10 x 3 with ones for the bias terms\n",
        "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
        "# Y Labels are 10 x 1\n",
        "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
        "\n",
        "# Apply gradient descent\n",
        "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
        "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMAi_Zv856Vb",
        "outputId": "d5d91a4d-a44d-45c7-e7a6-34cd46d01c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.66111809.\n",
            "The resulting vector of weights is [np.float64(-3.9e-07), np.float64(-0.0003557), np.float64(0.00039862)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-87cfadf101d0>:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  J = float(J)\n"
          ]
        }
      ],
      "source": [
        "# Check the gradient descent function\n",
        "np.random.seed(1)\n",
        "# X input is 20 x 3 with ones for the bias terms\n",
        "tmp_X = np.append(np.ones((20, 1)), np.random.rand(20, 2) * 2000, axis=1)\n",
        "# Y Labels are 20 x 1\n",
        "tmp_Y = (np.random.rand(20, 1) > 0.35).astype(float)\n",
        "\n",
        "# Apply gradient descent\n",
        "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 1000)\n",
        "# you can check gradient descent performance with different number of iterations (300, 500, 700, 1000, 1200, 1500)\n",
        "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNT-7veC56Vb"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Extracting the Features\n",
        "\n",
        "* Given a list of reviews, extract the features and store them in a matrix.\n",
        "    * The first feature is the number of positive words in a review.\n",
        "    * The second feature is the number of negative words in a review.\n",
        "* Then train your logistic regression classifier on these features.\n",
        "* Test the classifier on a validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps4qgTb_56Vb"
      },
      "outputs": [],
      "source": [
        "def extract_features(review, freqs, process_textt=process_text):\n",
        "\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_text(review)\n",
        "\n",
        "    # 3 elements for [bias, positive, negative] counts\n",
        "    x = np.zeros(3)\n",
        "\n",
        "    # bias term is set to 1\n",
        "    x[0] = 1\n",
        "\n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "\n",
        "        # increment the word count for the positive label 1\n",
        "        x[1] += freqs.get((word, 1),0)\n",
        "\n",
        "        # increment the word count for the negative label 0\n",
        "        x[2] += freqs.get((word, 0),0)\n",
        "\n",
        "    x = x[None, :]  # adding batch dimension for further processing\n",
        "    assert(x.shape == (1, 3))\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "negu4GSz56Vb",
        "outputId": "db6e94e8-efd8-41d7-a757-1a9168e5a404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.00000e+00 2.21321e+05 1.98587e+05]]\n"
          ]
        }
      ],
      "source": [
        "# Check your function\n",
        "tmp1 = extract_features(train_x[0], freqs)\n",
        "print(tmp1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DCQankN56Vb",
        "outputId": "1377a17c-d432-4cdb-f48e-36a70cddafa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# test 2:\n",
        "# check for when the words are not in the freqs dictionary\n",
        "tmp2 = extract_features('bfhuehv blfeej34b bloodecweb', freqs)\n",
        "print(tmp2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgdaZnIk56Vc"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Training Your Model\n",
        "\n",
        "To train the model:\n",
        "* Stack the features for all training examples into a matrix X.\n",
        "* Call `gradientDescent`, which you've implemented above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T6QQQ5J56Vc",
        "outputId": "b59086b8-48fd-4227-ec42-00f33aa54476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-87cfadf101d0>:24: RuntimeWarning: divide by zero encountered in log\n",
            "  J = -1/m*(np.dot(y.transpose(),np.log(h))+np.dot((1-y).transpose(),np.log(1-h)))\n",
            "<ipython-input-13-1e5aa93da050>:9: RuntimeWarning: overflow encountered in exp\n",
            "  h = 1/(1+np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is nan.\n",
            "The resulting vector of weights is [np.float64(2e-08), np.float64(0.00165088), np.float64(-0.00143872)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-87cfadf101d0>:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  J = float(J)\n"
          ]
        }
      ],
      "source": [
        "# Collect the features from each IMDB review and stack into matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :] = extract_features(train_x[i], freqs)\n",
        "\n",
        "# Training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent to learn weights (theta)\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 6500)\n",
        "\n",
        "# Print training cost and weights\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF7yKenZ56Vc"
      },
      "source": [
        "<a name='4'></a>\n",
        "## 4 -  Test your Logistic Regression\n",
        "\n",
        "Finally we have to test logistic regression.\n",
        "<a name='ex-4'></a>\n",
        "\n",
        "Implement `predict_review`.\n",
        "Predict whether a review is positive or negative.\n",
        "\n",
        "* Given a review, process it, then extract the features.\n",
        "* Apply the model's learned weights on the features to get the logits.\n",
        "* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n",
        "\n",
        "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuPaJ2jb56Vc"
      },
      "outputs": [],
      "source": [
        "def predict_review(review, freqs, theta):\n",
        "    '''\n",
        "    Input:\n",
        "        review: a string (IMDB review)\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output:\n",
        "        y_pred: the probability of a review being positive or negative\n",
        "    '''\n",
        "    # extract the features of the review\n",
        "    x = extract_features(review, freqs)\n",
        "\n",
        "    # make the prediction using x and theta\n",
        "    y_pred = sigmoid(np.dot(x, theta))\n",
        "\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up2Z_MOu56Vc",
        "outputId": "a0f89ca6-d6e6-43f3-d101-d46f66447b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This film was brilliant and heartwarming. -> 1.000000\n",
            "Absolutely terrible acting and a horrible plot. -> 0.000008\n"
          ]
        }
      ],
      "source": [
        "# Test example reviews (IMDB-style)\n",
        "for review in [\n",
        "    \"This film was brilliant and heartwarming.\",\n",
        "    \"Absolutely terrible acting and a horrible plot.\"\n",
        "\n",
        "]:\n",
        "    prediction = predict_review(review, freqs, theta)[0][0]\n",
        "    print(f\"{review} -> {prediction:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S10g0RJp56Vc",
        "outputId": "18c4e76f-b5cc-43ed-9f80-464984bb4ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am learning to appreciate great cinema. This film was a work of art. -> 1.000000\n",
            "Horrible movie. The plot made no sense and the ending was frustrating. -> 0.000006\n"
          ]
        }
      ],
      "source": [
        "# Your own reviews\n",
        "my_review = \"I am learning to appreciate great cinema. This film was a work of art.\"\n",
        "my_review_neg = \"Horrible movie. The plot made no sense and the ending was frustrating.\"\n",
        "\n",
        "print(f\"{my_review} -> {predict_review(my_review, freqs, theta)[0][0]:.6f}\")\n",
        "print(f\"{my_review_neg} -> {predict_review(my_review_neg, freqs, theta)[0][0]:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test ambiguous (uncertain) IMDB-style reviews\n",
        "ambiguous_reviews = [\n",
        "    \"The movie had some good moments, but overall it felt flat.\",\n",
        "    \"I liked the idea, but the execution could have been better.\",\n",
        "    \"Not the best film I've seen, but not the worst either.\",\n",
        "    \"Some scenes were beautiful, but I couldn't connect with the story.\",\n",
        "    \"It started strong, but the ending ruined it for me.\"\n",
        "]\n",
        "\n",
        "for review in ambiguous_reviews:\n",
        "    prediction = predict_review(review, freqs, theta)[0][0]\n",
        "    print(f\"{review} -> {prediction:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsQxzcd8If-W",
        "outputId": "ecfd0ab2-a8b3-4818-c0dd-5dde4eb2224e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The movie had some good moments, but overall it felt flat. -> 0.023868\n",
            "I liked the idea, but the execution could have been better. -> 0.001930\n",
            "Not the best film I've seen, but not the worst either. -> 1.000000\n",
            "Some scenes were beautiful, but I couldn't connect with the story. -> 0.999894\n",
            "It started strong, but the ending ruined it for me. -> 0.769231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPbri5vt56Vc"
      },
      "source": [
        "<a name='4-1'></a>\n",
        "### 4.1 -  Check the Performance using the Test Set\n",
        "After training your model using the training set above, we have to check the model accuracy\n",
        "\n",
        "<a name='ex-5'></a>\n",
        "\n",
        "Implement `test_logistic_regression`.\n",
        "* Given the test data and the weights of your trained model, calculate the accuracy of your logistic regression model.\n",
        "* Use your 'predict_tweet' function to make predictions on each tweet in the test set.\n",
        "* If the prediction is > 0.5, set the model's classification 'y_hat' to 1, otherwise set the model's classification 'y_hat' to 0.\n",
        "* A prediction is accurate when the y_hat equals the test_y.  Sum up all the instances when they are equal and divide by m.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrK8V0Qe56Vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d835c642-4996-4cdd-ef89-9e91f31919f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.6205\n"
          ]
        }
      ],
      "source": [
        "def test_logistic_regression(reviews, labels, freqs, theta, predict_review=predict_review):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        reviews: a list of IMDB reviews (test set)\n",
        "        labels: (m, 1) vector with the corresponding labels for the reviews\n",
        "        freqs: a dictionary with the frequency of each (word, label) pair\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output:\n",
        "        accuracy: (# of reviews classified correctly) / (total # of reviews)\n",
        "    \"\"\"\n",
        "    # List to store predictions\n",
        "    y_hat = []\n",
        "\n",
        "    for review in reviews:\n",
        "        # Get the prediction for the current review\n",
        "        y_pred = predict_review(review, freqs, theta)\n",
        "\n",
        "        # Apply threshold to convert probability to binary class\n",
        "        if y_pred > 0.5:\n",
        "            y_hat.append(1.0)\n",
        "        else:\n",
        "            y_hat.append(0.0)\n",
        "\n",
        "    # Convert predictions to (m,1) array for comparison\n",
        "    y_hat = np.array(y_hat)[:, np.newaxis]\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.sum(y_hat == labels) / len(labels)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Evaluate model on IMDB test set\n",
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VqCHrMp56Vc"
      },
      "source": [
        "Later in this specialization, we will see how we can use deeplearning to improve the prediction performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQn8kbKg56Vc"
      },
      "source": [
        "<a name='5'></a>\n",
        "## 5 - Predict with your own text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpD3iBWE56Vc",
        "outputId": "1481cec7-0cb1-4e3d-ce96-c2b8de6de3b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['expect', 'great', 'movi', 'bore', 'full', 'cliché']\n",
            "[[0.62862272]]\n",
            "Positive sentiment\n"
          ]
        }
      ],
      "source": [
        "\n",
        "my_review = \"I expected a great movie, but it was boring and full of clichés.\"\n",
        "\n",
        "print(process_text(my_review))\n",
        "y_hat = predict_review(my_review, freqs, theta)\n",
        "print(y_hat)\n",
        "\n",
        "\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else:\n",
        "    print('Negative sentiment')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of4zxjfw56Vc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}